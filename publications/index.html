<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Melinos Averkiou | publications</title>
    <meta name="author" content="Melinos  Averkiou" />
    <meta name="description" content="Melinos personal website
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🎓</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://melinos.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://melinos.github.io/">Melinos Averkiou</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="https://lodurality.github.io/GEM3D/static/images/teaser/03001627-205-0_render0001.png">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Petrov:2024:GEM3D" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis</div>
          <!-- Author -->
          <div class="author">Petrov, Dmitry, Goyal, Pradyumn, Thamizharasan, Vikas, Kim, Vladimir G., Gadelha, Matheus, 
                  <em>Averkiou, Melinos</em>, Chaudhuri, Siddhartha, and Kalogerakis, Evangelos
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proc. SIGGRAPH</em>, 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://lodurality.github.io/GEM3D/static/GEM3D_website.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://lodurality.github.io/GEM3D/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://lodurality.github.io/GEM3D/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce GEM3D — a new deep, topology-aware generative model of 3D shapes.
	The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. 
	Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), 
	then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information 
	stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations.
	We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. 
	We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios 
	of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/facadenet.png">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Georgiou:2024:FacadeNet" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">FacadeNet: Conditional Facade Synthesis via Selective Editing</div>
          <!-- Author -->
          <div class="author">Georgiou, Yiangos, Loizou, Marios, Kelly, Tom, and <em>Averkiou, Melinos</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proc. WACV</em>, 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2311.01240.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/ygeorg01/FacadeNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://ygeorg01.github.io/FacadeNet/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce FacadeNet, a deep learning approach for synthesizing building
		 	facade images from diverse viewpoints. Our method employs a conditional GAN, taking a
			   single view of a facade along with the desired viewpoint information and
			   generates an image of the facade from the distinct viewpoint. To
			   precisely modify view-dependent elements like windows and doors while
			   preserving the structure of view-independent components such as walls,
			   we introduce a selective editing module. This module leverages image
			   embeddings extracted from a pretrained vision transformer. Our
			   experiments demonstrated state-of-the-art performance on building
			   facade generation, surpassing alternative methods</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/CSN.jpeg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Loizou:2023:CSN" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Cross-Shape Attention for Part Segmentation of 3D Point Clouds</div>
          <!-- Author -->
          <div class="author">Loizou, M., Garg, S., Petrov, D., 
                  <em>Averkiou, M.</em>, and Kalogerakis, E.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Graphics Forum (Proc. SGP)</em>, , 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2003.09053.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/marios2019/CSN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://marios2019.github.io/CSN/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a deep learning method that propagates point-wise feature representations across shapes within
               a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to
               enable interactions between a shape’s point-wise features and those of other shapes. The mechanism assesses
               both the degree of interaction between points and also mediates feature propagation across shapes, improving
               the accuracy and consistency of the resulting point-wise feature representations for shape segmentation.
               Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention
               operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art
               results in the popular PartNet dataset.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/ANNFASS.png">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Artopoulos:2023:ANNFASS" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">An artificial neural network framework for classifying the style of cypriot hybrid examples of built heritage in 3D</div>
          <!-- Author -->
          <div class="author">Artopoulos, G., Maslioukova, M. I., Zavou, C., Loizou, M., Deligiorgi, M., and <em>Averkiou, M.</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Journal of Cultural Heritage</em>, 63, 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.researchgate.net/profile/Giorgos-Artopoulos/publication/373018277_An_artificial_neural_network_framework_for_classifying_the_style_of_Cypriot_hybrid_examples_of_built_heritage_in_3D/links/64d47feeb684851d3d9b159c/An-artificial-neural-network-framework-for-classifying-the-style-of-Cypriot-hybrid-examples-of-built-heritage-in-3D.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://annfass-srv.cs.ucy.ac.cy/home" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The article presents a workflow based on Deep Neural Networks (DNNs) and Support Vector Machine (SVM) for identifying
			   architectural stylistic influences of segmented building parts of Cypriot historical architecture in 3D. The research
			   contributes in the field of Digital Cultural Heritage (DCH) by applying Machine Learning (ML) and Deep Learning (DL)
			   on recently published DCH data, with the aim to accelerate the segmentation and annotation process of Historic Building
			   Information modelling (HBIM) that is currently based on time-consuming manual processes. The method presented works on reality
			   captured data by 3D documentation techniques, precisely, Terrestrial Laser Scanning (TLS) or Photogrammetry. This workflow was
			   developed to enable the operation of an online platform,11https://annfass-srv.cs.ucy.ac.cy. which also provides access to the
			   building data presented here. Ultimately, the results of the presented method are accessible to scholars and students via this
			   platform which provides multiple functionalities for researchers in the field to use.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/annfass.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Deligiorgi:2021:ANNFASS" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A 3D digitisation workflow for architecture-specific annotation of built heritage</div>
          <!-- Author -->
          <div class="author">Deligiorgi, Marissia, Maslioukova, Maria I., 
                  <em>Averkiou, Melinos</em>, Andreou, Andreas C., Selvaraju, Pratheba, Kalogerakis, Evangelos, Patow, Gustavo, Chrysanthou, Yiorgos, and Artopoulos, George
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Journal of Archaeological Science: Reports</em>, 37, 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S2352409X20305782?via%3Dihub" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Contemporary discourse points to the central role that heritage plays in the process of enabling groups of various 
			  cultural or ethnic background to strengthen their feeling of belonging and sharing in society. Safeguarding heritage 
			  is also valued highly in the priorities of the European Commission. As a result, there have been several long-term 
			  initiatives involving the digitisation, annotation and cataloguing of tangible cultural heritage in museums and 
			  collections. Specifically, for built heritage, a pressing challenge is that historical monuments such as buildings, 
			  temples, churches or city fortification infrastructures are hard to document due to their historic palimpsest; 
			  spatial transformations, actions of destruction, reuse of material, or continuous urban development that covers traces 
			  and changes the formal integrity and identity of a cultural heritage site. The ability to reason about a monument’s 
			  form is crucial for efficient documentation and cataloguing. This paper presents a 3D digitisation workflow through the 
			  involvement of reality capture technologies for the annotation and structure analysis of built heritage with the use of 
			  3D Convolutional Neural Networks (3D CNNs) for classification purposes. The presented workflow contributes a new approach 
			  to the identification of a building’s architectural components (e.g., arch, dome) and to the study of the stylistic influences 
			  (e.g., Gothic, Byzantine) of building parts. In doing so this workflow can assist in tracking a building’s history, identifying 
			  its construction period and comparing it to other buildings of the same period. This process can contribute to educational 
			  and research activities, as well as facilitate the automated classification of datasets in digital repositories for scholarly research in digital humanities.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/3dv_2021.png">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Georgiou:2021:PUT" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Projective Urban Texturing</div>
          <!-- Author -->
          <div class="author">Georgiou, Yiangos, 
                  <em>Averkiou, Melinos</em>, Kelly, Tom, and Kalogerakis, Evangelos
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proc. 3DV</em>, Dec 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2201.10938.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/ygeorg01/PUT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://ygeorg01.github.io/PUT/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
            <a href="https://www.youtube.com/watch?v=QN18pQsbXKQ&amp;ab_channel=YiangosGeorgiou" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper proposes a method for automatic generation of textures for 3D city meshes in immersive urban environments. 
			  Many recent pipelines capture or synthesize large quantities of city geometry using scanners or procedural modeling pipelines. 
			  Such geometry is intricate and realistic, however the generation of photo-realistic textures for such large scenes remains a problem. 
			  We propose to generate textures for input target 3D meshes driven by the textural style present in readily available datasets of 
			  panoramic photos capturing urban environments. Re-targeting such 2D datasets to 3D geometry is challenging because the underlying shape, 
			  size, and layout of the urban structures in the photos do not correspond to the ones in the target meshes. Photos also often have objects 
			  (e.g., trees, vehicles) that may not even be present in the target geometry. To address these issues we present a method, called Projective 
			  Urban Texturing (PUT), which re-targets textural style from real-world panoramic images to unseen urban meshes. PUT relies on contrastive 
			  and adversarial training of a neural architecture designed for unpaired image-to-texture translation. The generated textures are stored in 
			  a texture atlas applied to the target 3D mesh geometry. To promote texture consistency, PUT employs an iterative procedure in which texture 
			  synthesis is conditioned on previously generated, adjacent textures. We demonstrate both quantitative and qualitative evaluation of the generated textures.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/buildingnet.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Selvaraju:2021:BuildingNet" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">BuildingNet: Learning to Label 3D Buildings</div>
          <!-- Author -->
          <div class="author">Selvaraju, P., Nabail, M., Loizou, M., Maslioukova, M., 
                  <em>Averkiou, M.</em>, Andreou, A., Chaudhuri, S., and Kalogerakis, E.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proc. ICCV</em>, Dec 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2110.04955.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/buildingnet/buildingnet_dataset" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://buildingnet.org/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
            <a href="https://www.youtube.com/watch?v=rl30WJo_EBo&amp;ab_channel=EvangelosKalogerakis" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce BuildingNet: (a) a large-scale dataset of
			   3D building models whose exteriors are consistently labeled,
			   and (b) a graph neural network that labels building meshes
			   by analyzing spatial and structural relations of their geometric
			   primitives. To create our dataset, we used crowdsourcing combined
			   with expert guidance, resulting in 513K annotated mesh primitives,
			   grouped into 292K semantic part components across 2K building models.
			   The dataset covers several building categories, such as houses, 
			   churches, skyscrapers, town halls, libraries, and castles. We include 
			   a benchmark for evaluating mesh and point cloud labeling. Buildings
			   have more challenging structural complexity compared to objects in
			   existing benchmarks (e.g., ShapeNet, PartNet), thus, we hope that 
			   our dataset can nurture the development of algorithms that are able 
			   to cope with such large-scale geometric data for both vision and
			   graphics tasks e.g., 3D semantic segmentation, part-based generative
			   models, correspondences, texturing, and analysis of point cloud data
			   acquired from real-world buildings. Finally, we show that our mesh-based
			   graph neural network significantly improves performance over several
			   baselines for labeling 3D meshes. Our project page www.buildingnet.org
			   includes our dataset and code.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/pb_dgcnn.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Loizou:2020:PB-DGCNN" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Learning Part Boundaries from 3D Point Clouds</div>
          <!-- Author -->
          <div class="author">Loizou, Marios, 
                  <em>Averkiou, Melinos</em>, and Kalogerakis, Evangelos
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Graphics Forum (Proc. SGP)</em>, 39(5), Dec 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2007.07563.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/marios2019/learning_part_boundaries" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://marios2019.github.io/learning_part_boundaries/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
            <a href="https://www.youtube.com/watch?v=pyCZiK28nl0&amp;ab_channel=MariosLoizou" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a method that detects boundaries of parts in 3D shapes represented as point clouds. 
			  Our method is based on a graph convolutional network architecture that outputs a probability 
			  for a point to lie in an area that separates two or more parts in a 3D shape. Our boundary 
			  detector is quite generic: it can be trained to localize boundaries of semantic parts or 
			  geometric primitives commonly used in 3D modeling. Our experiments demonstrate that our method 
			  can extract more accurate boundaries that are closer to ground-truth ones compared to alternatives. 
			  We also demonstrate an application of our network to fine-grained semantic shape segmentation, where 
			  we also show improvements in terms of part labeling performance.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/3dv18.png">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Lin:2018:MatNet" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Learning Material-Aware Local Descriptors for 3D Shapes</div>
          <!-- Author -->
          <div class="author">Lin, Hubert, 
                  <em>Averkiou, Melinos</em>, Kalogerakis, Evangelos, Kovacs, Balazs, Ranade, Siddhant, Kim, Vladimir, Chaudhuri, Siddhartha, and Bala, Kavita
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proc. 3DV</em>, Sep 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/1810.08729.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Material understanding is critical for design, geometric modeling, and analysis of functional objects. 
			  We enable material-aware 3D shape analysis by employing a projective convolutional neural network 
			  architecture to learn material-aware descriptors from view-based representations of 3D points for 
			  point-wise material classification or material-aware retrieval. Unfortunately, only a small fraction 
			  of shapes in 3D repositories are labeled with physical materials, posing a challenge for learning methods. 
			  To address this challenge, we crowdsource a dataset of 3080 3D shapes with part-wise material labels. 
			  We focus on furniture models which exhibit interesting structure and material variability. In addition, 
			  we also contribute a high-quality expert-labeled benchmark of 115 shapes from Herman-Miller and IKEA for evaluation. 
			  We further apply a mesh-aware conditional random field, which incorporates rotational and reflective symmetries, 
			  to smooth our local material predictions across neighboring surface patches. We demonstrate the effectiveness of 
			  our learned descriptors for automatic texturing, material-aware retrieval, and physical simulation.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/cvpr17.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Kalogerakis:2017:ShapePFCN" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">3D Shape Segmentation with Projective Convolutional Networks</div>
          <!-- Author -->
          <div class="author">Kalogerakis, Evangelos, 
                  <em>Averkiou, Melinos</em>, Maji, Subhransu, and Chaudhuri, Siddhartha
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proc. CVPR</em>, Jul 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/1612.02808.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/kalov/ShapePFCN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://people.cs.umass.edu/kalo/papers/shapepfcn/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
            <a href="https://www.youtube.com/watch?v=zrd2uZjYbCw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper introduces a deep architecture for segmenting 3D objects into their labeled semantic parts. 
			  Our architecture combines image-based Fully Convolutional Networks (FCNs) and surface-based Conditional 
			  Random Fields (CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are used for 
			  efficient view-based reasoning about 3D object parts. Through a special projection layer, FCN outputs 
			  are effectively aggregated across multiple views and scales, then are projected onto the 3D object surfaces. 
			  Finally, a surface-based CRF combines the projected outputs with geometric consistency cues to yield coherent 
			  segmentations. The whole architecture (multi-view FCNs and CRF) is trained end-to-end. Our approach significantly 
			  outperforms the existing state-of-the-art methods in the currently largest segmentation benchmark (ShapeNet). 
			  Finally, we demonstrate promising segmentation results on noisy 3D shapes acquired from consumer-grade depth cameras.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/style.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Hu:2017:Style" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Co-Locating Style-Defining Elements on 3D Shapes</div>
          <!-- Author -->
          <div class="author">Hu, Ruizhen, Li, Wenchao, Kaick, Oliver Van, Huang, Hui, 
                  <em>Averkiou, Melinos</em>, Cohen-Or, Daniel, and Zhang, Hao
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>ACM TOG</em>, 36(3), Jun 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://vcc.tech/file/upload_file//0/58//weboem_informations/style.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://vcc.tech/file/upload_file//0/58//weboem_informations/Style_Datasets.zip" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://vcc.szu.edu.cn/research/2017/style/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce a method for co-locating style-defining elements over a set of 3D shapes. 
			  Our goal is to translate high-level style descriptions, such as “Ming” or “European” 
			  for furniture models, into explicit and localized regions over the geometric models 
			  that characterize each style. For each style, the set of style-defining elements 
			  is defined as the union of all the elements that are able to discriminate the style. 
			  Another property of the style-defining elements is that they are frequently occurring, 
			  reflecting shape characteristics that appear across multiple shapes of the same style. 
			  Given an input set of 3D shapes spanning multiple categories and styles, where the shapes 
			  are grouped according to their style labels, we perform a cross-category co-analysis of 
			  the shape set to learn and spatially locate a set of defining elements for each style. 
			  This is accomplished by first sampling a large number of candidate geometric elements 
			  and then iteratively applying feature selection to the candidates, to extract 
			  style-discriminating elements until no additional elements can be found. Thus, for each 
			  style label, we obtain sets of discriminative elements that together form the superset of 
			  defining elements for the style. We demonstrate that the co-location of style-defining 
			  elements allows us to solve problems such as style classification, and enables a variety 
			  of applications such as style-revealing view selection, style-aware sampling, and 
			  style-driven modeling for 3D shapes.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/eg2016.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Averkiou:2016:Autocorr" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Autocorrelation Descriptor for Efficient Co-Alignment of 3D Shape Collections</div>
          <!-- Author -->
          <div class="author">
                  <em>Averkiou, Melinos</em>, Kim, Vladimir G., and Mitra, Niloy J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Graphics Forum</em>, 35(1), Jun 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://geometry.cs.ucl.ac.uk/projects/2015/coalignment/paper_docs/coalignment_cgf15.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="http://geometry.cs.ucl.ac.uk/projects/2015/coalignment/paper_docs/coalignment_code_data.7z" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://geometry.cs.ucl.ac.uk/projects/2015/coalignment/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Abstract Co-aligning a collection of shapes to a consistent pose is a common problem 
			  in shape analysis with applications in shape matching, retrieval and visualization. 
			  We observe that resolving among some orientations is easier than others, for example, 
			  a common mistake for bicycles is to align front-to-back, while even the simplest 
			  algorithm would not erroneously pick orthogonal alignment. The key idea of our work 
			  is to analyse rotational autocorrelations of shapes to facilitate shape co-alignment. 
			  In particular, we use such an autocorrelation measure of individual shapes to decide 
			  which shape pairs might have well-matching orientations; and, if so, which configurations 
			  are likely to produce better alignments. This significantly prunes the number of alignments 
			  to be examined, and leads to an efficient, scalable algorithm that performs comparably to 
			  state-of-the-art techniques on benchmark data sets, but requires significantly fewer computations, 
			  resulting in 2–16× speed improvement in our tests.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/thesis.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Averkiou:2015:Phd" class="col-sm-8">
        <!-- Title -->
          <div class="title">Data-driven Modelling of Shape Structure</div>
          <!-- Author -->
          <div class="author">
                <em>Averkiou, Melinos</em>
          </div>
		  
		  <!-- Year --><div class="periodical">
            <em>University College London</em>, 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://geometry.cs.ucl.ac.uk/projects/2015/thesis_melinos/paper_docs/MelinosAverkiouPhdThesis.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In recent years, the study of shape structure has shown great promise, 
				  by taking steps towards exposing shape semantics and functionality 
				  to algorithms spanning a wide range of areas in computer graphics 
				  and vision. By shape structure, we refer to the set of parts that make a 
				  shape, the relations between these parts, and the ways in which they 
				  correspond and vary between shapes of the same family. These developments 
				  have been largely driven by the abundance of 3D data, with collections of 
				  3D models becoming increasingly prominent and websites such as Trimble 3D 
				  Warehouse offering millions of free 3D models to the public. The ability to 
				  use large amounts of data inside these shape collections for discovering shape 
				  structure has made novel approaches to acquisition, modelling, fabrication, and 
				  recognition of 3D objects possible. Discovering and modelling the structure of 
				  shapes using such data is therefore of great importance. In this thesis we address 
				  the problem of discovering and modelling shape structure from large, diverse and 
				  unorganized shape collections. Our hypothesis is that by using the large amounts 
				  of data inside such shape collections we can discover and model shape structure, 
				  and thus use such information to enable structure-aware tools for 3D modelling, 
				  including shape exploration, synthesis and editing. We make three key contributions. 
				  First, we propose an efficient algorithm for co-aligning large and diverse collections 
				  of shapes, to tackle the first challenge in detecting shape structure, which is to 
				  place shapes in a common coordinate frame. Then, we introduce a method to parameterize 
				  shapes in terms of locations and sizes of their parts, and we demonstrate its application 
				  to concurrently exploring a shape collection and synthesizing new shapes. Finally, we 
				  define a meta-representation for a shape family, which models the relations of shape parts 
				  to capture the main geometric characteristics of the family, and we demonstrate how it can 
				  be used to explore shape collections and intelligently edit shapes.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/gcpr15.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Zheng:2015:manifold" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Object Proposals Estimation in Depth Image Using Compact 3D Shape Manifolds</div>
          <!-- Author -->
          <div class="author">Zheng, Shuai, Prisacariu, Victor Adrian, 
                  <em>Averkiou, Melinos</em>, Cheng, Ming-Ming, Mitra, Niloy J., Shotton, Jamie, Torr, Philip H. S., and Rother, Carsten
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In German Conference on Pattern Recognition</em>, Jun 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://geometry.cs.ucl.ac.uk/projects/2015/depth_proposals/paper_docs/depth_proposal_gcpr2015.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/torrvision/Objectness" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Man-made objects, such as chairs, often have very large shape variations, making it challenging to detect 
			them. In this work we investigate the task of finding particular object shapes from a single depth image. 
			We tackle this task by exploiting the inherently low dimensionality in the object shape variations, 
			which we discover and encode as a compact shape space. Starting from any collection of 3D models, we first 
			train a low dimensional Gaussian Process Latent Variable Shape Space. We then sample this space, effectively 
			producing infinite amounts of shape variations, which are used for training. Additionally, to support fast and 
			accurate inference, we improve the standard 3D object category proposal generation pipeline by applying a shallow 
			convolutional neural network-based filtering stage. This combination leads to considerable improvements for proposal 
			generation, in both speed and accuracy. We compare our full system to previous state-of-the-art approaches, on four 
			different shape classes, and show a clear improvement.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2014</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/metarep.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Fish:2014:Metarep" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Meta-Representation of Shape Families</div>
          <!-- Author -->
          <div class="author">Fish, Noa, 
                  <em>Averkiou, Melinos</em>, Kaick, Oliver, Sorkine-Hornung, Olga, Cohen-Or, Daniel, and Mitra, Niloy J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>ACM TOG (Proc. SIGGRAPH)</em>, 33(4), Jul 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/metarep/paper_docs/metaRepresentation_sigg14.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="http://geometry.cs.ucl.ac.uk/projects/2014/meta-representation/downloads/MR_Code_Data.zip" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/metarep/metaRep_sigg14.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
            <a href="https://www.youtube.com/watch?v=JoLmZtC1TCM&amp;ab_channel=NiloyJ.Mitra" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce a meta-representation that represents the essence of a family of shapes. The meta-representation learns the 
			  configurations of shape parts that are common across the family, and encapsulates this knowledge with a system of geometric 
			  distributions that encode relative arrangements of parts. Thus, instead of predefined priors, what characterizes a shape family 
			  is directly learned from the set of input shapes. The meta-representation is constructed from a set of co-segmented shapes with 
			  known correspondence. It can then be used in several applications where we seek to preserve the identity of the shapes as members 
			  of the family. We demonstrate applications of the meta-representation in exploration of shape repositories, where interesting shape 
			  configurations can be examined in the set; guided editing, where models can be edited while maintaining their familial traits; and 
			  coupled editing, where several shapes can be collectively deformed by directly manipulating the distributions in the meta-representation. 
			  We evaluate the efficacy of the proposed representation on a variety of shape collections.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/eg14.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Zheng:2014:Reccuring" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Recurring part arrangements in shape collections</div>
          <!-- Author -->
          <div class="author">Zheng, Youyi, Cohen-Or, Daniel, 
                  <em>Averkiou, Melinos</em>, and Mitra, Niloy J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Graphics Forum (Proc. Eurographics)</em>, 33(2), Jul 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://www.cad.zju.edu.cn/home/zyy/docs/recurringArrangements_eg14.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/recurr_arrangment/recurr_arrangement_eg14.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
            <a href="https://www.youtube.com/watch?v=QeMVD3A-QMo&amp;ab_channel=NiloyJ.Mitra" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Abstract Extracting semantically related parts across models remains challenging, especially without supervision. 
			  The common approach is to co-analyze a model collection, while assuming the existence of descriptive geometric 
			  features that can directly identify related parts. In the presence of large shape variations, common geometric features, 
			  however, are no longer sufficiently descriptive. In this paper, we explore an indirect top-down approach, where instead of 
			  part geometry, part arrangements extracted from each model are compared. The key observation is that while a direct comparison 
			  of part geometry can be ambiguous, part arrangements, being higher level structures, remain consistent, and hence can be used 
			  to discover latent commonalities among semantically related shapes. We show that our indirect analysis leads to the detection 
			  of recurring arrangements of parts, which are otherwise difficult to discover in a direct unsupervised setting. We evaluate 
			  our algorithm on ground truth datasets and report advantages over geometric similarity-based bottom-up co-segmentation algorithms.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/shapesynth.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Averkiou:2014:Shapesynth" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">ShapeSynth: Parameterizing model collections for coupled shape exploration and synthesis</div>
          <!-- Author -->
          <div class="author">
                  <em>Averkiou, Melinos</em>, Kim, Vladimir G., Zheng, Youyi, and Mitra, Niloy J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Graphics Forum (Proc. Eurographics)</em>, 33(2), Jul 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/shape_synth/paper_docs/shapeSynth_eg14.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/shape_synth/paper_docs/shapesynth_source.zip" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/shape_synth/shapeSynth_eg14.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
            <a href="https://www.youtube.com/watch?v=2gYDAe1WFJY&amp;ab_channel=NiloyJ.Mitra" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Abstract Recent advances in modeling tools enable non-expert users to synthesize novel shapes by assembling parts extracted from model 
			 databases. A major challenge for these tools is to provide users with relevant parts, which is especially difficult for large repositories 
			 with significant geometric variations. In this paper we analyze unorganized collections of 3D models to facilitate explorative shape synthesis 
			 by providing high-level feedback of possible synthesizable shapes. By jointly analyzing arrangements and shapes of parts across models, 
			 we hierarchically embed the models into low-dimensional spaces. The user can then use the parameterization to explore the existing models 
			 by clicking in different areas or by selecting groups to zoom on specific shape clusters. More importantly, any point in the embedded space 
			 can be lifted to an arrangement of parts to provide an abstracted view of possible shape variations. The abstraction can further be realized 
			 by appropriately deforming parts from neighboring models to produce synthesized geometry. Our experiments show that users can rapidly generate 
			 plausible and diverse shapes using our system, which also performs favorably with respect to previous modeling tools.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->

      <div class="row">
        <div class="col-sm-2">
<img style="box-shadow:4px 4px 8px #000; border-radius:2.5%; width: 100%; max-width: 150px; height: auto" src="../assets/img/sda11.jpg">
			<!--">-->
</div>

        <!-- Entry bib key -->
        <div id="Averkiou:2011:Comparison" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Comparison of relative (mouse-like) and absolute (tablet-like) interaction with a large stereoscopic workspace</div>
          <!-- Author -->
          <div class="author">
                  <em>Averkiou, Melinos</em>, and Dodgson, Neil A.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Stereoscopic Displays and Applications XXII</em>, Jul 2011
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.researchgate.net/publication/241378239_Comparison_of_relative_mouse-like_and_absolute_tablet-like_interaction_with_a_large_stereoscopic_workspace" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://www.youtube.com/watch?v=32J0PxAe6Uc&amp;ab_channel=ZeebaTV" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We compare two different modes of interaction with a large stereoscopic display, where the physical
              pointing device is in a volume distinct from the display volume. In absolute mode, the physical pointer’s
              position exactly maps to the virtual pointer’s position in the display volume, analogous to a 2D graphics
              table and 2D screen. In relative mode, the connection between the physical pointer’s motion and the motion
              of the virtual pointer in the display volume is analogous to that obtained with a 2D mouse and 2D screen.
              Both statistical analysis and participants’ feedback indicated a strong preference for absolute mode over
              relative mode. This is in contrast to 2D displays where relative mode (mouse) is far more prevalent than
              absolute mode (tablet). We also compared head-tracking against no head-tracking. There was no
              statistically-significant advantage to using head-tracking, however almost all participants strongly
              favoured head-tracking.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Melinos  Averkiou. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HHH9XH4XYC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-HHH9XH4XYC');
  </script>
  </body>
</html>

