---
---

@string{CVPR={Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}
@string{ICCV={Proc. IEEE/CVF International Conference on Computer Vision (ICCV)}}
@string{THREEDV={Proc. International Conference on 3D Vision (3DV)}}
@string{TOG={ACM Transactions on Graphics}}
@string{SIGG={ACM Transactions on Graphics (Proc. SIGGRAPH)}}

@string{CGF={Computer Graphics Forum}}
@string{CGF_EG={Computer Graphics Forum (Proc. Eurographics)}}
@string{CGF_SGP={Computer Graphics Forum (Proc. SGP)}}

%%% 2023

@article{Loizou:2023:CSN,
  author    = {Loizou, M. and Garg, S. and Petrov, D. and Averkiou, M. and Kalogerakis, E.},
  title     = {Cross-Shape Attention for Part Segmentation of 3D Point Clouds},
  journal	= CGF_SGP,
  year      = {2023},
  abstract  = {We present a deep learning method that propagates point-wise feature representations across shapes within
               a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to
               enable interactions between a shape's point-wise features and those of other shapes. The mechanism assesses
               both the degree of interaction between points and also mediates feature propagation across shapes, improving
               the accuracy and consistency of the resulting point-wise feature representations for shape segmentation.
               Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention
               operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art
               results in the popular PartNet dataset.},
  website   = {https://marios2019.github.io/CSN/},
  code      = {https://github.com/marios2019/CSN},
  pdf		= {https://arxiv.org/pdf/2003.09053.pdf},
  img		= {../assets/img/CSN.jpeg},
  selected  = True
}

@article{Artopoulos:2023:ANNFASS,
	title = {An artificial neural network framework for classifying the style of cypriot hybrid examples of built heritage in 3D},
	journal = {Journal of Cultural Heritage},
	volume = {63},
	pages = {135-147},
	year = {2023},
	issn = {1296-2074},
	doi = {https://doi.org/10.1016/j.culher.2023.07.016},
	url = {https://www.sciencedirect.com/science/article/pii/S1296207423001395},
	author = {Georgios Artopoulos and Maria I. Maslioukova and Christina Zavou and Marios Loizou and Marissia Deligiorgi and Melinos Averkiou},
	keywords = {Deep learning, Convolutional neural networks, Built heritage, 3D reality capture of architecture, Style classification, Semantic annotation},
	abstract = {The article presents a workflow based on Deep Neural Networks (DNNs) and Support Vector Machine (SVM) for identifying architectural stylistic influences of segmented building parts of Cypriot historical architecture in 3D. The research contributes in the field of Digital Cultural Heritage (DCH) by applying Machine Learning (ML) and Deep Learning (DL) on recently published DCH data [1], with the aim to accelerate the segmentation and annotation process of Historic Building Information modelling (HBIM) that is currently based on time-consuming manual processes. The method presented works on reality captured data by 3D documentation techniques, precisely, Terrestrial Laser Scanning (TLS) or Photogrammetry. This workflow was developed to enable the operation of an online platform,11https://annfass-srv.cs.ucy.ac.cy. which also provides access to the building data presented here. Ultimately, the results of the presented method are accessible to scholars and students via this platform which provides multiple functionalities for researchers in the field to use.}
	website   = {https://annfass-srv.cs.ucy.ac.cy/home},
	pdf	= {https://www.researchgate.net/profile/Giorgos-Artopoulos/publication/373018277_An_artificial_neural_network_framework_for_classifying_the_style_of_Cypriot_hybrid_examples_of_built_heritage_in_3D/links/64d47feeb684851d3d9b159c/An-artificial-neural-network-framework-for-classifying-the-style-of-Cypriot-hybrid-examples-of-built-heritage-in-3D.pdf},
	img	= {https://api.annfass-srv.cs.ucy.ac.cy/monuments/public/16306725152311666332_22_Archaeological%20Research%20Unit%20Building_05.jpg},
	selected = True
}


%%% 2021

@article{Deligiorgi:2021:ANNFASS,
  title = {A 3D digitisation workflow for architecture-specific annotation of built heritage},
  journal = {Journal of Archaeological Science: Reports},
  volume = {37},
  pages = {102787},
  year = {2021},
  issn = {2352-409X},
  doi = {https://doi.org/10.1016/j.jasrep.2020.102787},
  url = {https://www.sciencedirect.com/science/article/pii/S2352409X20305782},
  author = {Marissia Deligiorgi and Maria I. Maslioukova and Melinos Averkiou and Andreas C. Andreou and Pratheba Selvaraju and 
			Evangelos Kalogerakis and Gustavo Patow and Yiorgos Chrysanthou and George Artopoulos},
  keywords = {Convolutional neural network, Monument, Architectural structure, 3D data, Deep learning},
  abstract = {Contemporary discourse points to the central role that heritage plays in the process of enabling groups of various 
			  cultural or ethnic background to strengthen their feeling of belonging and sharing in society. Safeguarding heritage 
			  is also valued highly in the priorities of the European Commission. As a result, there have been several long-term 
			  initiatives involving the digitisation, annotation and cataloguing of tangible cultural heritage in museums and 
			  collections. Specifically, for built heritage, a pressing challenge is that historical monuments such as buildings, 
			  temples, churches or city fortification infrastructures are hard to document due to their historic palimpsest; 
			  spatial transformations, actions of destruction, reuse of material, or continuous urban development that covers traces 
			  and changes the formal integrity and identity of a cultural heritage site. The ability to reason about a monument’s 
			  form is crucial for efficient documentation and cataloguing. This paper presents a 3D digitisation workflow through the 
			  involvement of reality capture technologies for the annotation and structure analysis of built heritage with the use of 
			  3D Convolutional Neural Networks (3D CNNs) for classification purposes. The presented workflow contributes a new approach 
			  to the identification of a building’s architectural components (e.g., arch, dome) and to the study of the stylistic influences 
			  (e.g., Gothic, Byzantine) of building parts. In doing so this workflow can assist in tracking a building’s history, identifying 
			  its construction period and comparing it to other buildings of the same period. This process can contribute to educational 
			  and research activities, as well as facilitate the automated classification of datasets in digital repositories for scholarly research in digital humanities.},
  pdf = {https://www.sciencedirect.com/science/article/abs/pii/S2352409X20305782?via%3Dihub},
  img  = {../assets/img/annfass.jpg},
  selected = false
}

@inproceedings{Georgiou:2021:PUT, 
  title={Projective Urban Texturing},
  author={Yiangos Georgiou and Melinos Averkiou and Tom Kelly and Evangelos Kalogerakis},
  booktitle= THREEDV,
  year={2021},
  pages = {1034-1043},
  keywords = {geometry;training;three-dimensional displays;shape;urban areas;pipelines;layout},
  doi = {10.1109/3DV53792.2021.00111},
  url = {https://doi.ieeecomputersociety.org/10.1109/3DV53792.2021.00111},
  month = {dec},
  abstract = {This paper proposes a method for automatic generation of textures for 3D city meshes in immersive urban environments. 
			  Many recent pipelines capture or synthesize large quantities of city geometry using scanners or procedural modeling pipelines. 
			  Such geometry is intricate and realistic, however the generation of photo-realistic textures for such large scenes remains a problem. 
			  We propose to generate textures for input target 3D meshes driven by the textural style present in readily available datasets of 
			  panoramic photos capturing urban environments. Re-targeting such 2D datasets to 3D geometry is challenging because the underlying shape, 
			  size, and layout of the urban structures in the photos do not correspond to the ones in the target meshes. Photos also often have objects 
			  (e.g., trees, vehicles) that may not even be present in the target geometry. To address these issues we present a method, called Projective 
			  Urban Texturing (PUT), which re-targets textural style from real-world panoramic images to unseen urban meshes. PUT relies on contrastive 
			  and adversarial training of a neural architecture designed for unpaired image-to-texture translation. The generated textures are stored in 
			  a texture atlas applied to the target 3D mesh geometry. To promote texture consistency, PUT employs an iterative procedure in which texture 
			  synthesis is conditioned on previously generated, adjacent textures. We demonstrate both quantitative and qualitative evaluation of the generated textures.},
  website = {https://ygeorg01.github.io/PUT/},
  code = {https://github.com/ygeorg01/PUT},
  pdf = {https://arxiv.org/pdf/2201.10938.pdf},
  img  = {../assets/img/3dv_2021.png},
  video = {https://www.youtube.com/watch?v=QN18pQsbXKQ&ab_channel=YiangosGeorgiou},
  selected = true
}

@inproceedings{Selvaraju:2021:BuildingNet, 
  author    = {Selvaraju, P. and Nabail, M. and Loizou, M. and Maslioukova, M. and
               Averkiou, M. and Andreou, A. and Chaudhuri, S. and Kalogerakis, E.},
  title     = {BuildingNet: Learning to Label 3D Buildings},
  booktitle = ICCV,
  year      = {2021},
  abstract  = {We introduce BuildingNet: (a) a large-scale dataset of
			   3D building models whose exteriors are consistently labeled,
			   and (b) a graph neural network that labels building meshes
			   by analyzing spatial and structural relations of their geometric
			   primitives. To create our dataset, we used crowdsourcing combined
			   with expert guidance, resulting in 513K annotated mesh primitives,
			   grouped into 292K semantic part components across 2K building models.
			   The dataset covers several building categories, such as houses, 
			   churches, skyscrapers, town halls, libraries, and castles. We include 
			   a benchmark for evaluating mesh and point cloud labeling. Buildings
			   have more challenging structural complexity compared to objects in
			   existing benchmarks (e.g., ShapeNet, PartNet), thus, we hope that 
			   our dataset can nurture the development of algorithms that are able 
			   to cope with such large-scale geometric data for both vision and
			   graphics tasks e.g., 3D semantic segmentation, part-based generative
			   models, correspondences, texturing, and analysis of point cloud data
			   acquired from real-world buildings. Finally, we show that our mesh-based
			   graph neural network significantly improves performance over several
			   baselines for labeling 3D meshes. Our project page www.buildingnet.org
			   includes our dataset and code.},
  website   = {https://buildingnet.org/},
  code      = {https://github.com/buildingnet/buildingnet_dataset},
  pdf		= {https://arxiv.org/pdf/2110.04955.pdf},
  img		= {../assets/img/buildingnet.jpg},
  video     = {https://www.youtube.com/watch?v=rl30WJo_EBo&ab_channel=EvangelosKalogerakis},
  selected  = True
} 

%%% 2020

@article{Loizou:2020:PB-DGCNN,
  author = {Marios Loizou and Melinos Averkiou and Evangelos Kalogerakis},
  title = {Learning Part Boundaries from 3D Point Clouds},
  journal = CGF_SGP,
  volume = {39},
  number = {5},
  pages = {183-195},
  keywords = {CCS Concepts, • Computing methodologies → Neural networks, Point-based models, Shape analysis},
  doi = {https://doi.org/10.1111/cgf.14078},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14078},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14078},
  abstract = {We present a method that detects boundaries of parts in 3D shapes represented as point clouds. 
			  Our method is based on a graph convolutional network architecture that outputs a probability 
			  for a point to lie in an area that separates two or more parts in a 3D shape. Our boundary 
			  detector is quite generic: it can be trained to localize boundaries of semantic parts or 
			  geometric primitives commonly used in 3D modeling. Our experiments demonstrate that our method 
			  can extract more accurate boundaries that are closer to ground-truth ones compared to alternatives. 
			  We also demonstrate an application of our network to fine-grained semantic shape segmentation, where 
			  we also show improvements in terms of part labeling performance.},
  year = {2020},
  website = {https://marios2019.github.io/learning_part_boundaries/},
  code = {https://github.com/marios2019/learning_part_boundaries},
  pdf = {https://arxiv.org/pdf/2007.07563.pdf},
  img = {../assets/img/pb_dgcnn.jpg},
  video = {https://www.youtube.com/watch?v=pyCZiK28nl0&ab_channel=MariosLoizou},
  selected  = true
}

%%% 2018

@inproceedings{Lin:2018:MatNet,
  author = {Hubert Lin and Melinos Averkiou and Evangelos Kalogerakis and Balazs Kovacs and 
			Siddhant Ranade and Vladimir Kim and Siddhartha Chaudhuri and Kavita Bala},
  booktitle = THREEDV,
  title = {Learning Material-Aware Local Descriptors for 3D Shapes},
  year = {2018},
  pages = {150-159},
  keywords = {shape;three-dimensional displays;metals;training;plastics;benchmark testing;geometry},
  doi = {10.1109/3DV.2018.00027},
  url = {https://doi.ieeecomputersociety.org/10.1109/3DV.2018.00027},
  month = {sep},
  abstract = {Material understanding is critical for design, geometric modeling, and analysis of functional objects. 
			  We enable material-aware 3D shape analysis by employing a projective convolutional neural network 
			  architecture to learn material-aware descriptors from view-based representations of 3D points for 
			  point-wise material classification or material-aware retrieval. Unfortunately, only a small fraction 
			  of shapes in 3D repositories are labeled with physical materials, posing a challenge for learning methods. 
			  To address this challenge, we crowdsource a dataset of 3080 3D shapes with part-wise material labels. 
			  We focus on furniture models which exhibit interesting structure and material variability. In addition, 
			  we also contribute a high-quality expert-labeled benchmark of 115 shapes from Herman-Miller and IKEA for evaluation. 
			  We further apply a mesh-aware conditional random field, which incorporates rotational and reflective symmetries, 
			  to smooth our local material predictions across neighboring surface patches. We demonstrate the effectiveness of 
			  our learned descriptors for automatic texturing, material-aware retrieval, and physical simulation.},
  pdf = {https://arxiv.org/pdf/1810.08729.pdf},
  img  = {../assets/img/3dv18.png},
  selected = true
}

%%% 2017

@inproceedings{Kalogerakis:2017:ShapePFCN,
  author = {Evangelos Kalogerakis and Melinos Averkiou and Subhransu Maji and Siddhartha Chaudhuri},
  booktitle = CVPR,
  title = {3D Shape Segmentation with Projective Convolutional Networks},
  year = {2017},
  issn = {1063-6919},
  pages = {6630-6639},
  keywords = {shape;three-dimensional displays;image segmentation;surface treatment;labeling;cognition},
  doi = {10.1109/CVPR.2017.702},
  url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.702},
  month = {jul},
  abstract = {This paper introduces a deep architecture for segmenting 3D objects into their labeled semantic parts. 
			  Our architecture combines image-based Fully Convolutional Networks (FCNs) and surface-based Conditional 
			  Random Fields (CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are used for 
			  efficient view-based reasoning about 3D object parts. Through a special projection layer, FCN outputs 
			  are effectively aggregated across multiple views and scales, then are projected onto the 3D object surfaces. 
			  Finally, a surface-based CRF combines the projected outputs with geometric consistency cues to yield coherent 
			  segmentations. The whole architecture (multi-view FCNs and CRF) is trained end-to-end. Our approach significantly 
			  outperforms the existing state-of-the-art methods in the currently largest segmentation benchmark (ShapeNet). 
			  Finally, we demonstrate promising segmentation results on noisy 3D shapes acquired from consumer-grade depth cameras.},
  website = {https://people.cs.umass.edu/kalo/papers/shapepfcn/index.html},
  code = {https://github.com/kalov/ShapePFCN},
  pdf = {https://arxiv.org/pdf/1612.02808.pdf},
  img  = {../assets/img/cvpr17.jpg},
  video = {https://www.youtube.com/watch?v=zrd2uZjYbCw},
  selected = true
}

@article{Hu:2017:Style,
  author = {Ruizhen Hu and Wenchao Li and Oliver Van Kaick and Hui Huang and Melinos Averkiou and Daniel Cohen-Or and Hao Zhang},
  title = {Co-Locating Style-Defining Elements on 3D Shapes},
  year = {2017},
  issue_date = {June 2017},
  volume = {36},
  number = {3},
  issn = {0730-0301},
  url = {https://doi.org/10.1145/3092817},
  doi = {10.1145/3092817},
  abstract = {We introduce a method for co-locating style-defining elements over a set of 3D shapes. 
			  Our goal is to translate high-level style descriptions, such as “Ming” or “European” 
			  for furniture models, into explicit and localized regions over the geometric models 
			  that characterize each style. For each style, the set of style-defining elements 
			  is defined as the union of all the elements that are able to discriminate the style. 
			  Another property of the style-defining elements is that they are frequently occurring, 
			  reflecting shape characteristics that appear across multiple shapes of the same style. 
			  Given an input set of 3D shapes spanning multiple categories and styles, where the shapes 
			  are grouped according to their style labels, we perform a cross-category co-analysis of 
			  the shape set to learn and spatially locate a set of defining elements for each style. 
			  This is accomplished by first sampling a large number of candidate geometric elements 
			  and then iteratively applying feature selection to the candidates, to extract 
			  style-discriminating elements until no additional elements can be found. Thus, for each 
			  style label, we obtain sets of discriminative elements that together form the superset of 
			  defining elements for the style. We demonstrate that the co-location of style-defining 
			  elements allows us to solve problems such as style classification, and enables a variety 
			  of applications such as style-revealing view selection, style-aware sampling, and 
			  style-driven modeling for 3D shapes.},
  journal = TOG,
  month = {jun},
  articleno = {33},
  numpages = {15},
  keywords = {style-aware sampling, Style elements},
  website = {http://vcc.szu.edu.cn/research/2017/style/},
  code = {https://vcc.tech/file/upload_file//0/58//weboem_informations/Style_Datasets.zip},
  pdf = {https://vcc.tech/file/upload_file//0/58//weboem_informations/style.pdf},
  img  = {../assets/img/style.jpg},
  selected = true
}

%%% 2016

@article{Averkiou:2016:Autocorr,
  author = {Melinos Averkiou and Vladimir G. Kim and Niloy J. Mitra},
  title = {Autocorrelation Descriptor for Efficient Co-Alignment of 3D Shape Collections},
  journal = CGF,
  volume = {35},
  number = {1},
  pages = {261-271},
  keywords = {digital geometry processing, modeling, I.3.3 Computer Graphics: Computational Geometry and Object Modelling—Geometric algorithms},
  doi = {https://doi.org/10.1111/cgf.12723},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12723},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12723},
  abstract = {Abstract Co-aligning a collection of shapes to a consistent pose is a common problem 
			  in shape analysis with applications in shape matching, retrieval and visualization. 
			  We observe that resolving among some orientations is easier than others, for example, 
			  a common mistake for bicycles is to align front-to-back, while even the simplest 
			  algorithm would not erroneously pick orthogonal alignment. The key idea of our work 
			  is to analyse rotational autocorrelations of shapes to facilitate shape co-alignment. 
			  In particular, we use such an autocorrelation measure of individual shapes to decide 
			  which shape pairs might have well-matching orientations; and, if so, which configurations 
			  are likely to produce better alignments. This significantly prunes the number of alignments 
			  to be examined, and leads to an efficient, scalable algorithm that performs comparably to 
			  state-of-the-art techniques on benchmark data sets, but requires significantly fewer computations, 
			  resulting in 2–16× speed improvement in our tests.},
  year = {2016},
  website = {http://geometry.cs.ucl.ac.uk/projects/2015/coalignment/},
  code = {http://geometry.cs.ucl.ac.uk/projects/2015/coalignment/paper_docs/coalignment_code_data.7z},
  pdf = {http://geometry.cs.ucl.ac.uk/projects/2015/coalignment/paper_docs/coalignment_cgf15.pdf},
  img  = {../assets/img/eg2016.jpg},
  selected = false
}

%%% 2015

@thesis{Averkiou:2015:Phd,
  author       = {Melinos Averkiou}, 
  title        = {Data-driven Modelling of Shape Structure},
  school       = {University College London},
  year         = {2015},
  month        = {aug},
  abstract     = {In recent years, the study of shape structure has shown great promise, 
				  by taking steps towards exposing shape semantics and functionality 
				  to algorithms spanning a wide range of areas in computer graphics 
				  and vision. By shape structure, we refer to the set of parts that make a 
				  shape, the relations between these parts, and the ways in which they 
				  correspond and vary between shapes of the same family. These developments 
				  have been largely driven by the abundance of 3D data, with collections of 
				  3D models becoming increasingly prominent and websites such as Trimble 3D 
				  Warehouse offering millions of free 3D models to the public. The ability to 
				  use large amounts of data inside these shape collections for discovering shape 
				  structure has made novel approaches to acquisition, modelling, fabrication, and 
				  recognition of 3D objects possible. Discovering and modelling the structure of 
				  shapes using such data is therefore of great importance. In this thesis we address 
				  the problem of discovering and modelling shape structure from large, diverse and 
				  unorganized shape collections. Our hypothesis is that by using the large amounts 
				  of data inside such shape collections we can discover and model shape structure, 
				  and thus use such information to enable structure-aware tools for 3D modelling, 
				  including shape exploration, synthesis and editing. We make three key contributions. 
				  First, we propose an efficient algorithm for co-aligning large and diverse collections 
				  of shapes, to tackle the first challenge in detecting shape structure, which is to 
				  place shapes in a common coordinate frame. Then, we introduce a method to parameterize 
				  shapes in terms of locations and sizes of their parts, and we demonstrate its application 
				  to concurrently exploring a shape collection and synthesizing new shapes. Finally, we 
				  define a meta-representation for a shape family, which models the relations of shape parts 
				  to capture the main geometric characteristics of the family, and we demonstrate how it can 
				  be used to explore shape collections and intelligently edit shapes.},
  pdf = {http://geometry.cs.ucl.ac.uk/projects/2015/thesis_melinos/paper_docs/MelinosAverkiouPhdThesis.pdf},
  img  = {../assets/img/thesis.jpg},
  selected = false
}

@inproceedings{Zheng:2015:manifold,
  author={Zheng, Shuai and Prisacariu, Victor Adrian and Averkiou, Melinos and Cheng, Ming-Ming and Mitra, Niloy J. 
		  and Shotton, Jamie and Torr, Philip H. S. and Rother, Carsten},
  editor={Gall, Juergen and Gehler, Peter and Leibe, Bastian},
  title={Object Proposals Estimation in Depth Image Using Compact 3D Shape Manifolds},
  booktitle={German Conference on Pattern Recognition},
  year={2015},
  pages={196--208},
  abstract={Man-made objects, such as chairs, often have very large shape variations, making it challenging to detect 
			them. In this work we investigate the task of finding particular object shapes from a single depth image. 
			We tackle this task by exploiting the inherently low dimensionality in the object shape variations, 
			which we discover and encode as a compact shape space. Starting from any collection of 3D models, we first 
			train a low dimensional Gaussian Process Latent Variable Shape Space. We then sample this space, effectively 
			producing infinite amounts of shape variations, which are used for training. Additionally, to support fast and 
			accurate inference, we improve the standard 3D object category proposal generation pipeline by applying a shallow 
			convolutional neural network-based filtering stage. This combination leads to considerable improvements for proposal 
			generation, in both speed and accuracy. We compare our full system to previous state-of-the-art approaches, on four 
			different shape classes, and show a clear improvement.},
  isbn={978-3-319-24947-6},
  code = {https://github.com/torrvision/Objectness},
  pdf = {http://geometry.cs.ucl.ac.uk/projects/2015/depth_proposals/paper_docs/depth_proposal_gcpr2015.pdf},
  img  = {../assets/img/gcpr15.jpg},
  selected = false
}

%%% 2014

@article{Fish:2014:Metarep,
  author = {Fish, Noa and Averkiou, Melinos and van Kaick, Oliver and Sorkine-Hornung, Olga and Cohen-Or, Daniel and Mitra, Niloy J.},
  title = {Meta-Representation of Shape Families},
  year = {2014},
  issue_date = {July 2014},
  volume = {33},
  number = {4},
  issn = {0730-0301},
  url = {https://doi.org/10.1145/2601097.2601185},
  doi = {10.1145/2601097.2601185},
  abstract = {We introduce a meta-representation that represents the essence of a family of shapes. The meta-representation learns the 
			  configurations of shape parts that are common across the family, and encapsulates this knowledge with a system of geometric 
			  distributions that encode relative arrangements of parts. Thus, instead of predefined priors, what characterizes a shape family 
			  is directly learned from the set of input shapes. The meta-representation is constructed from a set of co-segmented shapes with 
			  known correspondence. It can then be used in several applications where we seek to preserve the identity of the shapes as members 
			  of the family. We demonstrate applications of the meta-representation in exploration of shape repositories, where interesting shape 
			  configurations can be examined in the set; guided editing, where models can be edited while maintaining their familial traits; and 
			  coupled editing, where several shapes can be collectively deformed by directly manipulating the distributions in the meta-representation. 
			  We evaluate the efficacy of the proposed representation on a variety of shape collections.},
  journal = SIGG,
  month = {jul},
  articleno = {34},
  numpages = {11},
  keywords = {consensus relations, shape collections, model editing},
  website = {http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/metarep/metaRep_sigg14.html},
  pdf = {http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/metarep/paper_docs/metaRepresentation_sigg14.pdf},
  code = {http://geometry.cs.ucl.ac.uk/projects/2014/meta-representation/downloads/MR_Code_Data.zip},
  img  = {../assets/img/metarep.jpg},
  video = {https://www.youtube.com/watch?v=JoLmZtC1TCM&ab_channel=NiloyJ.Mitra},
  selected = true
}

@article{Zheng:2014:Reccuring,
  author = {Zheng, Youyi and Cohen-Or, Daniel and Averkiou, Melinos and Mitra, Niloy J.},
  title = {Recurring part arrangements in shape collections},
  journal = CGF_EG,
  volume = {33},
  number = {2},
  pages = {115-124},
  doi = {https://doi.org/10.1111/cgf.12309},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12309},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12309},
  abstract = {Abstract Extracting semantically related parts across models remains challenging, especially without supervision. 
			  The common approach is to co-analyze a model collection, while assuming the existence of descriptive geometric 
			  features that can directly identify related parts. In the presence of large shape variations, common geometric features, 
			  however, are no longer sufficiently descriptive. In this paper, we explore an indirect top-down approach, where instead of 
			  part geometry, part arrangements extracted from each model are compared. The key observation is that while a direct comparison 
			  of part geometry can be ambiguous, part arrangements, being higher level structures, remain consistent, and hence can be used 
			  to discover latent commonalities among semantically related shapes. We show that our indirect analysis leads to the detection 
			  of recurring arrangements of parts, which are otherwise difficult to discover in a direct unsupervised setting. We evaluate 
			  our algorithm on ground truth datasets and report advantages over geometric similarity-based bottom-up co-segmentation algorithms.},
  year = {2014},
  website = {http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/recurr_arrangment/recurr_arrangement_eg14.html},
  pdf = {http://www.cad.zju.edu.cn/home/zyy/docs/recurringArrangements_eg14.pdf},
  img  = {../assets/img/eg14.jpg},
  video = {https://www.youtube.com/watch?v=QeMVD3A-QMo&ab_channel=NiloyJ.Mitra},
  selected = false
}

@article{Averkiou:2014:Shapesynth,
  author = {Averkiou, Melinos and Kim, Vladimir G. and Zheng, Youyi and Mitra, Niloy J.},
  title = {ShapeSynth: Parameterizing model collections for coupled shape exploration and synthesis},
  journal = CGF_EG,
  volume = {33},
  number = {2},
  pages = {125-134},
  doi = {https://doi.org/10.1111/cgf.12310},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12310},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12310},
  abstract = {Abstract Recent advances in modeling tools enable non-expert users to synthesize novel shapes by assembling parts extracted from model 
			 databases. A major challenge for these tools is to provide users with relevant parts, which is especially difficult for large repositories 
			 with significant geometric variations. In this paper we analyze unorganized collections of 3D models to facilitate explorative shape synthesis 
			 by providing high-level feedback of possible synthesizable shapes. By jointly analyzing arrangements and shapes of parts across models, 
			 we hierarchically embed the models into low-dimensional spaces. The user can then use the parameterization to explore the existing models 
			 by clicking in different areas or by selecting groups to zoom on specific shape clusters. More importantly, any point in the embedded space 
			 can be lifted to an arrangement of parts to provide an abstracted view of possible shape variations. The abstraction can further be realized 
			 by appropriately deforming parts from neighboring models to produce synthesized geometry. Our experiments show that users can rapidly generate 
			 plausible and diverse shapes using our system, which also performs favorably with respect to previous modeling tools.},
  year = {2014},
  website = {http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/shape_synth/shapeSynth_eg14.html},
  code = {http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/shape_synth/paper_docs/shapesynth_source.zip},
  pdf = {http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/shape_synth/paper_docs/shapeSynth_eg14.pdf},
  img  = {../assets/img/shapesynth.jpg},
  video = {https://www.youtube.com/watch?v=2gYDAe1WFJY&ab_channel=NiloyJ.Mitra},
  selected = false
}

%%% 2011

@inproceedings{Averkiou:2011:Comparison,
  author = {Melinos Averkiou and Neil A. Dodgson},
  title = {Comparison of relative (mouse-like) and absolute (tablet-like) interaction with a large stereoscopic workspace},
  abstract = {We compare two different modes of interaction with a large stereoscopic display, where the physical
              pointing device is in a volume distinct from the display volume. In absolute mode, the physical pointer's
              position exactly maps to the virtual pointer's position in the display volume, analogous to a 2D graphics
              table and 2D screen. In relative mode, the connection between the physical pointer's motion and the motion
              of the virtual pointer in the display volume is analogous to that obtained with a 2D mouse and 2D screen.
              Both statistical analysis and participants' feedback indicated a strong preference for absolute mode over
              relative mode. This is in contrast to 2D displays where relative mode (mouse) is far more prevalent than
              absolute mode (tablet). We also compared head-tracking against no head-tracking. There was no
              statistically-significant advantage to using head-tracking, however almost all participants strongly
              favoured head-tracking.},
  volume = {7863},
  booktitle = {Stereoscopic Displays and Applications XXII},
  editor = {Andrew J. Woods and Nicolas S. Holliman and Neil A. Dodgson},
  organization = {International Society for Optics and Photonics},
  publisher = {SPIE},
  pages = {381 -- 392},
  keywords = {stereoscopic, mouse, tablet, interaction, head-tracking},
  year = {2011},
  doi = {10.1117/12.872002},
  URL = {https://doi.org/10.1117/12.872002},
  pdf = {https://www.researchgate.net/publication/241378239_Comparison_of_relative_mouse-like_and_absolute_tablet-like_interaction_with_a_large_stereoscopic_workspace},
  img  = {../assets/img/sda11.jpg},
  video = {https://www.youtube.com/watch?v=32J0PxAe6Uc&ab_channel=ZeebaTV},
  selected = false
}




